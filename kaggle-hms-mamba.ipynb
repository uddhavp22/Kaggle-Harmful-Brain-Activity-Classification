{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":59093,"databundleVersionId":7469972,"sourceType":"competition"}],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"Imports\n","metadata":{}},{"cell_type":"code","source":"!pip install antropy\n\nimport torch\n\nimport mne\n\nimport antropy\n\n\nfrom sklearn.preprocessing import label_binarize\n\nmne.set_log_level('ERROR')\n\nif torch.cuda.is_available():\n    device = \"cuda\"\nelse:\n    device = \"cpu\"\nimport pandas as pd\n\nimport numpy as np\nfrom scipy.fftpack import fft, ifft\nfrom scipy.stats import entropy\nfrom sklearn.preprocessing import LabelEncoder\nfrom scipy.stats import skew\nfrom scipy.stats import kurtosis\n#NOTE USE LABEL ENCODER BEFORE RUNNING THE FULL VERSION ","metadata":{"execution":{"iopub.status.busy":"2024-04-07T13:42:59.350854Z","iopub.execute_input":"2024-04-07T13:42:59.351179Z","iopub.status.idle":"2024-04-07T13:43:11.574360Z","shell.execute_reply.started":"2024-04-07T13:42:59.351142Z","shell.execute_reply":"2024-04-07T13:43:11.573181Z"},"trusted":true},"execution_count":51,"outputs":[{"name":"stdout","text":"Requirement already satisfied: antropy in /opt/conda/lib/python3.10/site-packages (0.1.6)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from antropy) (1.26.4)\nRequirement already satisfied: scipy in /opt/conda/lib/python3.10/site-packages (from antropy) (1.11.4)\nRequirement already satisfied: scikit-learn in /opt/conda/lib/python3.10/site-packages (from antropy) (1.2.2)\nRequirement already satisfied: numba>=0.57 in /opt/conda/lib/python3.10/site-packages (from antropy) (0.58.1)\nRequirement already satisfied: stochastic in /opt/conda/lib/python3.10/site-packages (from antropy) (0.7.0)\nRequirement already satisfied: llvmlite<0.42,>=0.41.0dev0 in /opt/conda/lib/python3.10/site-packages (from numba>=0.57->antropy) (0.41.1)\nRequirement already satisfied: joblib>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from scikit-learn->antropy) (1.3.2)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from scikit-learn->antropy) (3.2.0)\n","output_type":"stream"}]},{"cell_type":"markdown","source":"Dataset and preprocessing\n","metadata":{}},{"cell_type":"code","source":"#this is where we actually extract the features \ndef get_variability_measures(eeg_data):\n    variability_features=[]\n    std_value=np.std(eeg_data,axis=1)\n    iqr_value = np.subtract(*np.percentile(eeg_data, [75, 25], axis=1))  # IQR of all channels\n    variability_features.extend((np.mean(std_value),np.mean(iqr_value)))\n    return variability_features\n\ndef get_distribution_features(eeg_data):\n    distribution_features=[]\n    skewness_value = skew(eeg_data, axis=1)\n    kurtosis_value = kurtosis(eeg_data, axis=1) \n    distribution_features.extend((np.mean(skewness_value),np.mean(kurtosis_value)))\n    return distribution_features\n  \ndef zero_crossings(signal):\n    # If signal crosses zero line, we'll have a change in sign of adjacent values\n    return ((signal[:-1] * signal[1:]) < 0).sum()\n\ndef frequency_content_features(eeg_data):\n    # Calculate zero crossings for each channel\n    zero_crossings_values = np.apply_along_axis(zero_crossings, 1, eeg_data)\n    \n    mean_zero_crossings = np.mean(zero_crossings_values)\n    return mean_zero_crossings\n\ndef hjorth_mobility(signal):\n    first_derivative = np.diff(signal)\n    variance = np.var(signal)\n    variance_derivative = np.var(first_derivative)\n    mobility = np.sqrt(variance_derivative / variance)\n    return mobility\n\ndef get_hjorth_parameters(eeg_data):\n    hjorth_parameters=[]\n    signal=eeg_data\n    \n    \n    \n    #hjorth complexity\n    first_derivative = np.diff(signal)\n    second_derivative = np.diff(first_derivative)\n    mobility = hjorth_mobility(signal)\n    mobility_derivative = hjorth_mobility(first_derivative)\n    complexity = mobility_derivative / mobility\n    hjorth_parameters.extend((mobility,complexity))\n    return hjorth_parameters\n\ndef get_higuchi_fractal_dimension(eeg_data):\n    signal=eeg_data\n    kmax=6\n    Lk = []\n    for k in range(1, kmax):\n        Lm = []\n        for m in range(0, k):\n            Lmk = 0\n            for i in range(1, int(np.floor((len(signal) - m) / k))):\n                Lmk += abs(signal[m + i * k] - signal[m + i * k - k])\n            Lmk = Lmk * (len(signal) / (k * int(np.floor((len(signal) - m) / k))))\n            Lm.append(Lmk)\n        Lk.append(np.log(np.mean(Lm)))\n    (slope, _) = np.polyfit(np.log(range(1, kmax)), Lk, 1)\n    return -slope\n\ndef get_entropies(eeg_data):\n    entropies=[]\n    signal=eeg_data\n    #shanon entropy\n    hist, _ = np.histogram(signal, bins=64, density=True)\n    shannon_entropy=entropy(hist)\n    #spectral entropy\n    spectral_entropy=antropy.spectral_entropy(eeg_data,200,'welch',nperseg=None,normalize=True)\n    #binned_entorpy\n    hist, _ = np.histogram(signal, bins=10)\n    binned_entropy=entropy(hist)\n    entropies.extend((shannon_entropy,spectral_entropy,binned_entropy))\n    return entropies\n    \ndef get_power_bands(new_raw,tmin,tmax):\n    \n    power_features=[]\n    \n\n# Define frequency bands\n    bands = {\n        'delta': (0.5, 4),\n        'theta': (4, 8),\n        'alpha': (8, 13),\n        'beta': (13, 30),\n        'gamma': (30, None)  # Assuming Gamma is 30+ Hz\n    }\n\n# Dictionary to hold power values for each band\n    band_power = {}\n\n    for band, (l_freq, h_freq) in bands.items():\n        # Filter the data for each frequency band\n        band_data = new_raw.copy().filter(l_freq=l_freq, h_freq=h_freq, picks='eeg', verbose=False)\n\n        # Compute the PSD for the filtered data using the Welch method\n        sfreq = raw.info['sfreq'] \n        \n        spectrum = band_data.compute_psd(method='welch', fmin=l_freq, fmax=np.inf,picks='eeg',verbose=False)\n\n        psd=spectrum.get_data()\n        # Integrate the PSD over the frequencies of interest to get the absolute power for each band\n        band_power[band] = psd.mean(axis=1).sum()\n\n\n\n    \n    power_features.extend(band_power.values())\n    \n    epsilon = 1e-6\n\n    # Calculate power ratios using values from the dictionary\n    delta_theta_ratio = band_power['delta'] / (band_power['theta'] + epsilon)\n    delta_alpha_ratio = band_power['delta'] / (band_power['alpha'] + epsilon)\n    theta_beta_ratio = band_power['theta'] / (band_power['beta'] + epsilon)\n    alpha_gamma_ratio = band_power['alpha'] / (band_power['gamma'] + epsilon)\n    beta_gamma_ratio = band_power['beta'] / (band_power['gamma'] + epsilon)\n\n    # Append the calculated ratios \n    power_features.extend([delta_theta_ratio, delta_alpha_ratio, theta_beta_ratio, alpha_gamma_ratio, beta_gamma_ratio])\n\n    # Calculate power sums using values from the dictionary\n    delta_theta_sum = band_power['delta'] + band_power['theta']\n    alpha_beta_sum = band_power['alpha'] + band_power['beta']\n    theta_gamma_sum = band_power['theta'] + band_power['gamma']\n\n    # Append the calculated sums \n    power_features.extend([delta_theta_sum, alpha_beta_sum, theta_gamma_sum])\n    total_power=band_power['delta'] + band_power['theta']+ band_power['gamma']+band_power['alpha']\n    power_features.append(total_power)\n    return power_features\n    \n\n    \n\n    ","metadata":{"execution":{"iopub.status.busy":"2024-04-07T14:10:26.136949Z","iopub.execute_input":"2024-04-07T14:10:26.137306Z","iopub.status.idle":"2024-04-07T14:10:26.162237Z","shell.execute_reply.started":"2024-04-07T14:10:26.137280Z","shell.execute_reply":"2024-04-07T14:10:26.161233Z"},"trusted":true},"execution_count":79,"outputs":[]},{"cell_type":"code","source":"\n    \ndef extract_features(surrogate_segment, segment_info,tmin,tmax):\n    features = []\n    eeg_data = np.real(surrogate_segment)\n    new_raw = mne.io.RawArray(eeg_data, segment_info)\n    \n    \n    features.extend(get_variability_measures(eeg_data))\n    features.extend(get_distribution_features(eeg_data))\n    \n    \n    zero_crossing_feature = [frequency_content_features(eeg_data)]\n    features.extend(zero_crossing_feature)\n    \n    # Extend with placeholders for missing features (Hjorth Mobility, etc.)\n    features.extend(get_hjorth_parameters(eeg_data)) \n    \n    higuchi_fractal_dimension=[get_higuchi_fractal_dimension(eeg_data)]\n    features.extend(higuchi_fractal_dimension)\n    \n    features.extend(get_entropies(eeg_data))\n    power_band_features = get_power_bands(new_raw,tmin,tmax)\n    features.extend(power_band_features)\n    \n    \n    \n    \n    return features\n\n\ndef create_raw_from_parquet(parquet_file):\n    df=pd.read_parquet(parquet_file)\n    data=df.to_numpy().T\n    info=mne.create_info(ch_names=list(df.columns),sfreq=200,ch_types='eeg')\n    raw=mne.io.RawArray(data,info)\n    return raw\n\ndef get_duration(raw):\n    num_samples=len(raw.times)\n    sampling_freq=raw.info['sfreq']\n    duration=np.floor(num_samples/sampling_freq)\n    \n    return duration","metadata":{"execution":{"iopub.status.busy":"2024-04-07T14:10:29.900227Z","iopub.execute_input":"2024-04-07T14:10:29.901234Z","iopub.status.idle":"2024-04-07T14:10:29.910738Z","shell.execute_reply.started":"2024-04-07T14:10:29.901198Z","shell.execute_reply":"2024-04-07T14:10:29.909712Z"},"trusted":true},"execution_count":80,"outputs":[]},{"cell_type":"code","source":"\n\n\n# Initialize an empty DataFrame for the new training data\ncolumns = [\n    'eeg_id', \n    'eeg_sub_id', \n    'patient_id', \n    'Standard Deviation (STD)', \n    'Inter-Quartile Range (IQR)', \n    'Skewness', \n    'Kurtosis', \n    'Number of Zero Crossings', \n    'Hjorth Mobility', \n    'Hjorth Complexity', \n    'Higuchi Fractal Dimension', \n    'Shannon Entropy', \n    'Spectral Entropy', \n    'Binned Entropy', \n    'Delta Power', \n    'Theta Power', \n    'Alpha Power', \n    'Beta Power', \n    'Gamma Power', \n    'Delta/Theta Ratio', \n    'Delta/Alpha Ratio', \n    'Theta/Beta Ratio', \n    'Alpha/Gamma Ratio', \n    'Beta/Gamma Ratio', \n    'Delta+Theta Power', \n    'Alpha+Beta Power', \n    'Theta+Gamma Power', \n    'Total Power',\n    'expert_consensus'\n]\n\nnew_train_df = pd.DataFrame(columns=columns)\n#print(new_train_df.shape[1])\n\n# Load the original training CSV\ntrain_csv = pd.read_csv('/kaggle/input/hms-harmful-brain-activity-classification/train.csv')\ncount=0\n# Loop through each row in the train CSV\nfor index, row in train_csv.iterrows():\n    eeg_id = row['eeg_id']\n    sub_id = row['eeg_sub_id']\n    patient_id = row['patient_id']\n    seizure_label = row['expert_consensus']\n   \n    # Load the corresponding parquet file as an mne Raw object\n    raw = create_raw_from_parquet(f'/kaggle/input/hms-harmful-brain-activity-classification/train_eegs/{eeg_id}.parquet')\n    \n    sub_id_start_time=sub_id\n    tmin = sub_id_start_time * 50\n    tmax = tmin + 50  # This ensures a 50-second window\n\n    # Adjust tmax to not exceed the recording\n    tmax = min(tmax, raw.times[-1])\n    \n    # Additionally, ensure tmin does not exceed the adjusted tmax or the recording\n    tmin = min(tmin, tmax - 0.001)\n    segment = raw.crop(tmin, tmax)  # Adjust 'tmin' and 'tmax' as necessary\n    segment_info=raw.info\n    # Generate FT Surrogate for the segment\n    ft_segment = fft(segment.get_data())\n    random_phases = np.exp(2j * np.pi * np.random.rand(*ft_segment.shape))\n    surrogate_data = np.abs(ft_segment) * random_phases\n    surrogate_segment = ifft(surrogate_data)\n    \n    # Extract features from the surrogate segment\n    features = extract_features(surrogate_segment,segment_info,tmin,tmax)\n    #print(len(features))\n    # Append to the new DataFrame\n    new_row = [eeg_id, sub_id, patient_id] + features + [seizure_label]\n    #print(len(new_row))\n    new_train_df.loc[len(new_train_df)] = new_row\n    ''''count+=1\n    print(count)\n    if count==1:\n        break'''\n# Save the new DataFrame to CSV\nnew_train_df.to_csv('feature_extracted_data.csv', index=False)\n","metadata":{"execution":{"iopub.status.busy":"2024-04-07T14:10:32.707438Z","iopub.execute_input":"2024-04-07T14:10:32.708296Z","iopub.status.idle":"2024-04-07T14:10:33.515935Z","shell.execute_reply.started":"2024-04-07T14:10:32.708262Z","shell.execute_reply":"2024-04-07T14:10:33.514716Z"},"trusted":true},"execution_count":81,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/numpy/core/fromnumeric.py:3787: RuntimeWarning: Degrees of freedom <= 0 for slice\n  return _methods._var(a, axis=axis, dtype=dtype, out=out, ddof=ddof,\n/opt/conda/lib/python3.10/site-packages/numpy/core/_methods.py:163: RuntimeWarning: invalid value encountered in divide\n  arrmean = um.true_divide(arrmean, div, out=arrmean,\n/opt/conda/lib/python3.10/site-packages/numpy/core/_methods.py:198: RuntimeWarning: invalid value encountered in scalar divide\n  ret = ret.dtype.type(ret / rcount)\n/opt/conda/lib/python3.10/site-packages/scipy/signal/_spectral_py.py:589: UserWarning: nperseg = 256 is greater than input length  = 1, using nperseg = 1\n  freqs, _, Pxy = _spectral_helper(x, y, fs, window, nperseg, noverlap,\n/opt/conda/lib/python3.10/site-packages/antropy/entropy.py:253: RuntimeWarning: invalid value encountered in divide\n  psd_norm = psd / psd.sum(axis=axis, keepdims=True)\n/opt/conda/lib/python3.10/site-packages/antropy/entropy.py:256: RuntimeWarning: invalid value encountered in divide\n  se /= np.log2(psd_norm.shape[axis])\n/tmp/ipykernel_33/1856721939.py:98: RuntimeWarning: filter_length (1321) is longer than the signal (1), distortion is likely. Reduce filter length or filter a longer signal.\n  band_data = new_raw.copy().filter(l_freq=l_freq, h_freq=h_freq, picks='eeg', verbose=False)\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","Cell \u001b[0;32mIn[81], line 68\u001b[0m\n\u001b[1;32m     65\u001b[0m surrogate_segment \u001b[38;5;241m=\u001b[39m ifft(surrogate_data)\n\u001b[1;32m     67\u001b[0m \u001b[38;5;66;03m# Extract features from the surrogate segment\u001b[39;00m\n\u001b[0;32m---> 68\u001b[0m features \u001b[38;5;241m=\u001b[39m \u001b[43mextract_features\u001b[49m\u001b[43m(\u001b[49m\u001b[43msurrogate_segment\u001b[49m\u001b[43m,\u001b[49m\u001b[43msegment_info\u001b[49m\u001b[43m,\u001b[49m\u001b[43mtmin\u001b[49m\u001b[43m,\u001b[49m\u001b[43mtmax\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     69\u001b[0m \u001b[38;5;66;03m#print(len(features))\u001b[39;00m\n\u001b[1;32m     70\u001b[0m \u001b[38;5;66;03m# Append to the new DataFrame\u001b[39;00m\n\u001b[1;32m     71\u001b[0m new_row \u001b[38;5;241m=\u001b[39m [eeg_id, sub_id, patient_id] \u001b[38;5;241m+\u001b[39m features \u001b[38;5;241m+\u001b[39m [seizure_label]\n","Cell \u001b[0;32mIn[80], line 21\u001b[0m, in \u001b[0;36mextract_features\u001b[0;34m(surrogate_segment, segment_info, tmin, tmax)\u001b[0m\n\u001b[1;32m     18\u001b[0m features\u001b[38;5;241m.\u001b[39mextend(higuchi_fractal_dimension)\n\u001b[1;32m     20\u001b[0m features\u001b[38;5;241m.\u001b[39mextend(get_entropies(eeg_data))\n\u001b[0;32m---> 21\u001b[0m power_band_features \u001b[38;5;241m=\u001b[39m \u001b[43mget_power_bands\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnew_raw\u001b[49m\u001b[43m,\u001b[49m\u001b[43mtmin\u001b[49m\u001b[43m,\u001b[49m\u001b[43mtmax\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     22\u001b[0m features\u001b[38;5;241m.\u001b[39mextend(power_band_features)\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m features\n","Cell \u001b[0;32mIn[79], line 103\u001b[0m, in \u001b[0;36mget_power_bands\u001b[0;34m(new_raw, tmin, tmax)\u001b[0m\n\u001b[1;32m    100\u001b[0m \u001b[38;5;66;03m# Compute the PSD for the filtered data using the Welch method\u001b[39;00m\n\u001b[1;32m    101\u001b[0m sfreq \u001b[38;5;241m=\u001b[39m raw\u001b[38;5;241m.\u001b[39minfo[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msfreq\u001b[39m\u001b[38;5;124m'\u001b[39m] \n\u001b[0;32m--> 103\u001b[0m spectrum \u001b[38;5;241m=\u001b[39m \u001b[43mband_data\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_psd\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mwelch\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfmin\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43ml_freq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfmax\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minf\u001b[49m\u001b[43m,\u001b[49m\u001b[43mpicks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43meeg\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    105\u001b[0m psd\u001b[38;5;241m=\u001b[39mspectrum\u001b[38;5;241m.\u001b[39mget_data()\n\u001b[1;32m    106\u001b[0m \u001b[38;5;66;03m# Integrate the PSD over the frequencies of interest to get the absolute power for each band\u001b[39;00m\n","File \u001b[0;32m<decorator-gen-176>:10\u001b[0m, in \u001b[0;36mcompute_psd\u001b[0;34m(self, method, fmin, fmax, tmin, tmax, picks, exclude, proj, remove_dc, reject_by_annotation, n_jobs, verbose, **method_kw)\u001b[0m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/mne/io/base.py:2184\u001b[0m, in \u001b[0;36mBaseRaw.compute_psd\u001b[0;34m(self, method, fmin, fmax, tmin, tmax, picks, exclude, proj, remove_dc, reject_by_annotation, n_jobs, verbose, **method_kw)\u001b[0m\n\u001b[1;32m   2181\u001b[0m method \u001b[38;5;241m=\u001b[39m _validate_method(method, \u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m)\n\u001b[1;32m   2182\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_set_legacy_nfft_default(tmin, tmax, method, method_kw)\n\u001b[0;32m-> 2184\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mSpectrum\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2185\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2186\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2187\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfmin\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfmin\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2188\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfmax\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfmax\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2189\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtmin\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtmin\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2190\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtmax\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtmax\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2191\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpicks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpicks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2192\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexclude\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexclude\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2193\u001b[0m \u001b[43m    \u001b[49m\u001b[43mproj\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2194\u001b[0m \u001b[43m    \u001b[49m\u001b[43mremove_dc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mremove_dc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2195\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreject_by_annotation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreject_by_annotation\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2196\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2197\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2198\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmethod_kw\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2199\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/mne/time_frequency/spectrum.py:1172\u001b[0m, in \u001b[0;36mSpectrum.__init__\u001b[0;34m(self, inst, method, fmin, fmax, tmin, tmax, picks, exclude, proj, remove_dc, reject_by_annotation, n_jobs, verbose, **method_kw)\u001b[0m\n\u001b[1;32m   1170\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minst\u001b[38;5;241m.\u001b[39mdata[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_picks][:, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_time_mask]\n\u001b[1;32m   1171\u001b[0m \u001b[38;5;66;03m# compute the spectra\u001b[39;00m\n\u001b[0;32m-> 1172\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_compute_spectra\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfmin\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfmax\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmethod_kw\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1173\u001b[0m \u001b[38;5;66;03m# check for correct shape and bad values\u001b[39;00m\n\u001b[1;32m   1174\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_values()\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/mne/time_frequency/spectrum.py:464\u001b[0m, in \u001b[0;36mBaseSpectrum._compute_spectra\u001b[0;34m(self, data, fmin, fmax, n_jobs, method_kw, verbose)\u001b[0m\n\u001b[1;32m    462\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_compute_spectra\u001b[39m(\u001b[38;5;28mself\u001b[39m, data, fmin, fmax, n_jobs, method_kw, verbose):\n\u001b[1;32m    463\u001b[0m     \u001b[38;5;66;03m# make the spectra\u001b[39;00m\n\u001b[0;32m--> 464\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_psd_func\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    465\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msfreq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfmin\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfmin\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfmax\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfmax\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\n\u001b[1;32m    466\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    467\u001b[0m     \u001b[38;5;66;03m# assign ._data (handling unaggregated multitaper output)\u001b[39;00m\n\u001b[1;32m    468\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_returns_complex_tapers(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmethod_kw):\n","File \u001b[0;32m<decorator-gen-146>:10\u001b[0m, in \u001b[0;36mpsd_array_welch\u001b[0;34m(x, sfreq, fmin, fmax, n_fft, n_overlap, n_per_seg, n_jobs, average, window, remove_dc, output, verbose)\u001b[0m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/mne/time_frequency/psd.py:205\u001b[0m, in \u001b[0;36mpsd_array_welch\u001b[0;34m(x, sfreq, fmin, fmax, n_fft, n_overlap, n_per_seg, n_jobs, average, window, remove_dc, output, verbose)\u001b[0m\n\u001b[1;32m    203\u001b[0m freq_mask \u001b[38;5;241m=\u001b[39m (freqs \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m fmin) \u001b[38;5;241m&\u001b[39m (freqs \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m fmax)\n\u001b[1;32m    204\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m freq_mask\u001b[38;5;241m.\u001b[39many():\n\u001b[0;32m--> 205\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo frequencies found between fmin=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfmin\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m and fmax=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfmax\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    206\u001b[0m freq_sl \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mslice\u001b[39m(\u001b[38;5;241m*\u001b[39m(np\u001b[38;5;241m.\u001b[39mwhere(freq_mask)[\u001b[38;5;241m0\u001b[39m][[\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]] \u001b[38;5;241m+\u001b[39m [\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m]))\n\u001b[1;32m    207\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m freq_mask\n","\u001b[0;31mValueError\u001b[0m: No frequencies found between fmin=0.5 and fmax=inf"],"ename":"ValueError","evalue":"No frequencies found between fmin=0.5 and fmax=inf","output_type":"error"}]},{"cell_type":"code","source":"print(new_train_df)","metadata":{"execution":{"iopub.status.busy":"2024-04-07T13:51:21.200904Z","iopub.execute_input":"2024-04-07T13:51:21.201632Z","iopub.status.idle":"2024-04-07T13:51:21.216761Z","shell.execute_reply.started":"2024-04-07T13:51:21.201601Z","shell.execute_reply":"2024-04-07T13:51:21.215745Z"},"trusted":true},"execution_count":71,"outputs":[{"name":"stdout","text":"       eeg_id  eeg_sub_id  patient_id  Standard Deviation (STD)  \\\n0  1628180742           0       42516                 30.275588   \n\n   Inter-Quartile Range (IQR)  Skewness  Kurtosis  Number of Zero Crossings  \\\n0                   40.444734  0.051111 -0.078174                    291.65   \n\n   Hjorth Mobility  Hjorth Complexity  ...  Delta/Theta Ratio  \\\n0         0.223952           4.905116  ...           0.691598   \n\n   Delta/Alpha Ratio Theta/Beta Ratio  Alpha/Gamma Ratio  Beta/Gamma Ratio  \\\n0           0.729855         0.540029           1.112028          2.173113   \n\n   Delta+Theta Power  Alpha+Beta Power  Theta+Gamma Power  Total Power  \\\n0         116.075163        192.086658         127.089996   239.568331   \n\n   expert_consensus  \n0           Seizure  \n\n[1 rows x 29 columns]\n","output_type":"stream"}]},{"cell_type":"code","source":"import pandas as pd\n\n# Load the CSV file\ndf = pd.read_csv('/kaggle/input/hms-harmful-brain-activity-classification/train.csv')\n\n# Calculate the percentage of each label\nlabel_percentage = df['expert_consensus'].value_counts(normalize=True) * 100\n\nprint(label_percentage)\n","metadata":{"execution":{"iopub.status.busy":"2024-04-07T13:51:30.980146Z","iopub.execute_input":"2024-04-07T13:51:30.981047Z","iopub.status.idle":"2024-04-07T13:51:31.150226Z","shell.execute_reply.started":"2024-04-07T13:51:30.981012Z","shell.execute_reply":"2024-04-07T13:51:31.149142Z"},"trusted":true},"execution_count":72,"outputs":[{"name":"stdout","text":"expert_consensus\nSeizure    19.600187\nGRDA       17.660112\nOther      17.610487\nGPD        15.638577\nLRDA       15.580524\nLPD        13.910112\nName: proportion, dtype: float64\n","output_type":"stream"}]},{"cell_type":"markdown","source":"Model Architure\n","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}