{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":59093,"databundleVersionId":7469972,"sourceType":"competition"}],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"Imports\n","metadata":{}},{"cell_type":"code","source":"\n\nimport torch\n\nimport mne\n\n\n\n\nfrom sklearn.preprocessing import label_binarize\n\nmne.set_log_level('ERROR')\n\nif torch.cuda.is_available():\n    device = \"cuda\"\nelse:\n    device = \"cpu\"\nimport pandas as pd\n\nimport numpy as np\nfrom scipy.fftpack import fft, ifft\nfrom sklearn.preprocessing import LabelEncoder\nfrom scipy.stats import skew\nfrom scipy.stats import kurtosis\n#NOTE USE LABEL ENCODER BEFORE RUNNING THE FULL VERSION ","metadata":{"execution":{"iopub.status.busy":"2024-04-07T12:51:25.260971Z","iopub.execute_input":"2024-04-07T12:51:25.261347Z","iopub.status.idle":"2024-04-07T12:51:25.267320Z","shell.execute_reply.started":"2024-04-07T12:51:25.261304Z","shell.execute_reply":"2024-04-07T12:51:25.266451Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"markdown","source":"Dataset and preprocessing\n","metadata":{}},{"cell_type":"code","source":"#this is where we actually extract the features \ndef get_variability_measures(eeg_data):\n    variability_features=[]\n    std_value=np.std(eeg_data,axis=1)\n    iqr_value = np.subtract(*np.percentile(eeg_data, [75, 25], axis=1))  # IQR of all channels\n    variability_features.extend((np.mean(std_value),np.mean(iqr_value)))\n    return variability_features\n\ndef get_distribution_features(eeg_data):\n    distribution_features=[]\n    skewness_value = skew(eeg_data, axis=1)\n    kurtosis_value = kurtosis(eeg_data, axis=1) \n    distribution_features.extend((np.mean(skewness_value),np.mean(kurtosis_value)))\n    return distribution_features\n  \ndef zero_crossings(signal):\n    # If signal crosses zero line, we'll have a change in sign of adjacent values\n    return ((signal[:-1] * signal[1:]) < 0).sum()\n\ndef frequency_content_features(eeg_data):\n    zero_crossings_values = np.apply_along_axis(zero_crossings, 1, eeg_data)\n    return zero_crossings_values\n\n\n    \ndef get_power_bands(new_raw):\n    \n    power_features=[]\n    \n\n# Define frequency bands\n    bands = {\n        'delta': (0.5, 4),\n        'theta': (4, 8),\n        'alpha': (8, 13),\n        'beta': (13, 30),\n        'gamma': (30, None)  # Assuming Gamma is 30+ Hz\n    }\n\n# Dictionary to hold power values for each band\n    band_power = {}\n\n    for band, (l_freq, h_freq) in bands.items():\n        # Filter the data for each frequency band\n        band_data = new_raw.copy().filter(l_freq=l_freq, h_freq=h_freq, picks='eeg', verbose=False)\n\n        # Compute the PSD for the filtered data using the Welch method\n        \n        spectrum= band_data.compute_psd(method='welch', fmin=l_freq, fmax=100, picks='eeg', verbose=False)\n        psd=spectrum.get_data()\n        # Integrate the PSD over the frequencies of interest to get the absolute power for each band\n        band_power[band] = psd.mean(axis=1).sum()\n\n\n\n    \n    power_features.extend(band_power.values())\n    \n    epsilon = 1e-6\n\n    # Calculate power ratios using values from the dictionary\n    delta_theta_ratio = band_power['delta'] / (band_power['theta'] + epsilon)\n    delta_alpha_ratio = band_power['delta'] / (band_power['alpha'] + epsilon)\n    theta_beta_ratio = band_power['theta'] / (band_power['beta'] + epsilon)\n    alpha_gamma_ratio = band_power['alpha'] / (band_power['gamma'] + epsilon)\n    beta_gamma_ratio = band_power['beta'] / (band_power['gamma'] + epsilon)\n\n    # Append the calculated ratios \n    power_features.extend([delta_theta_ratio, delta_alpha_ratio, theta_beta_ratio, alpha_gamma_ratio, beta_gamma_ratio])\n\n    # Calculate power sums using values from the dictionary\n    delta_theta_sum = band_power['delta'] + band_power['theta']\n    alpha_beta_sum = band_power['alpha'] + band_power['beta']\n    theta_gamma_sum = band_power['theta'] + band_power['gamma']\n\n    # Append the calculated sums \n    power_features.extend([delta_theta_sum, alpha_beta_sum, theta_gamma_sum])\n    \n    return power_features\n    \n\n    \n\n    ","metadata":{"execution":{"iopub.status.busy":"2024-04-07T12:51:29.850311Z","iopub.execute_input":"2024-04-07T12:51:29.851149Z","iopub.status.idle":"2024-04-07T12:51:29.865726Z","shell.execute_reply.started":"2024-04-07T12:51:29.851120Z","shell.execute_reply":"2024-04-07T12:51:29.864765Z"},"trusted":true},"execution_count":25,"outputs":[]},{"cell_type":"code","source":"def extract_features(surrogate_segment,segment_info):\n    \n    features=[]\n    eeg_data=np.real(surrogate_segment)\n    new_raw=mne.io.RawArray(eeg_data,segment_info)\n    \n    features.append(get_variability_measures(eeg_data))\n    print('no features should be 2 here ',len(features))\n    features.append(get_distribution_features(eeg_data))\n    print('no features should be 4 here ',len(features))\n    features.append(frequency_content_features(eeg_data))\n    print('no features should be 5 here ',len(features))\n    features.extend((1,2,3,4,5,6))\n    print('no features should be 11 here ',len(features))\n    features.append(get_power_bands(new_raw))\n    print(\"no features should be 24 here\",len(features))\n    features.append(0)\n    print(\"no features should be 25 hre\" ,len(features))\n    return features\n\ndef create_raw_from_parquet(parquet_file):\n    df=pd.read_parquet(parquet_file)\n    data=df.to_numpy().T\n    info=mne.create_info(ch_names=list(df.columns),sfreq=200,ch_types='eeg')\n    raw=mne.io.RawArray(data,info)\n    return raw\n\ndef get_duration(raw):\n    num_samples=len(raw.times)\n    sampling_freq=raw.info['sfreq']\n    duration=np.floor(num_samples/sampling_freq)\n    \n    return duration","metadata":{"execution":{"iopub.status.busy":"2024-04-07T12:51:33.650883Z","iopub.execute_input":"2024-04-07T12:51:33.651234Z","iopub.status.idle":"2024-04-07T12:51:33.661062Z","shell.execute_reply.started":"2024-04-07T12:51:33.651207Z","shell.execute_reply":"2024-04-07T12:51:33.660148Z"},"trusted":true},"execution_count":26,"outputs":[]},{"cell_type":"code","source":"\n\n\n# Initialize an empty DataFrame for the new training data\ncolumns = [\n    'eeg_id', \n    'eeg_sub_id', \n    'patient_id', \n    'Standard Deviation (STD)', \n    'Inter-Quartile Range (IQR)', \n    'Skewness', \n    'Kurtosis', \n    'Number of Zero Crossings', \n    'Hjorth Mobility', \n    'Hjorth Complexity', \n    'Higuchi Fractal Dimension', \n    'Shannon Entropy', \n    'Spectral Entropy', \n    'Binned Entropy', \n    'Delta Power', \n    'Theta Power', \n    'Alpha Power', \n    'Beta Power', \n    'Gamma Power', \n    'Delta/Theta Ratio', \n    'Delta/Alpha Ratio', \n    'Theta/Beta Ratio', \n    'Alpha/Gamma Ratio', \n    'Beta/Gamma Ratio', \n    'Delta+Theta Power', \n    'Alpha+Beta Power', \n    'Theta+Gamma Power', \n    'Total Power',\n    'expert_consensus'\n]\n\nnew_train_df = pd.DataFrame(columns=columns)\nprint(new_train_df.shape[1])\n\n# Load the original training CSV\ntrain_csv = pd.read_csv('/kaggle/input/hms-harmful-brain-activity-classification/train.csv')\ncount=0\n# Loop through each row in the train CSV\nfor index, row in train_csv.iterrows():\n    eeg_id = row['eeg_id']\n    sub_id = row['eeg_sub_id']\n    patient_id = row['patient_id']\n    seizure_label = row['expert_consensus']\n   \n    # Load the corresponding parquet file as an mne Raw object\n    raw = create_raw_from_parquet(f'/kaggle/input/hms-harmful-brain-activity-classification/train_eegs/{eeg_id}.parquet')\n    \n    sub_id_start_time=sub_id\n    tmin = sub_id_start_time * 50\n    tmax = tmin + 50  # This ensures a 50-second window\n\n    # Adjust tmax to not exceed the recording\n    tmax = min(tmax, raw.times[-1])\n    \n    # Additionally, ensure tmin does not exceed the adjusted tmax or the recording\n    tmin = min(tmin, tmax - 0.001)\n    segment = raw.crop(tmin, tmax)  # Adjust 'tmin' and 'tmax' as necessary\n    segment_info=raw.info\n    # Generate FT Surrogate for the segment\n    ft_segment = fft(segment.get_data())\n    random_phases = np.exp(2j * np.pi * np.random.rand(*ft_segment.shape))\n    surrogate_data = np.abs(ft_segment) * random_phases\n    surrogate_segment = ifft(surrogate_data)\n    \n    # Extract features from the surrogate segment\n    features = extract_features(surrogate_segment,segment_info)\n    print(len(features))\n    # Append to the new DataFrame\n    new_row = [eeg_id, sub_id, patient_id] + features + [seizure_label]\n    print(len(new_row))\n    new_train_df.loc[len(new_train_df)] = new_row\n    count+=1\n    print(count)\n    if count==1:\n        break\n# Save the new DataFrame to CSV\n#new_train_df.to_csv('path/to/new_train_data.csv', index=False)\n","metadata":{"execution":{"iopub.status.busy":"2024-04-07T12:51:38.199059Z","iopub.execute_input":"2024-04-07T12:51:38.199919Z","iopub.status.idle":"2024-04-07T12:51:38.726671Z","shell.execute_reply.started":"2024-04-07T12:51:38.199887Z","shell.execute_reply":"2024-04-07T12:51:38.725389Z"},"trusted":true},"execution_count":27,"outputs":[{"name":"stdout","text":"29\nno features should be 2 here  1\nno features should be 4 here  2\nno features should be 5 here  3\nno features should be 11 here  9\nno features should be 24 here 10\nno features should be 25 hre 11\n11\n15\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","Cell \u001b[0;32mIn[27], line 73\u001b[0m\n\u001b[1;32m     71\u001b[0m new_row \u001b[38;5;241m=\u001b[39m [eeg_id, sub_id, patient_id] \u001b[38;5;241m+\u001b[39m features \u001b[38;5;241m+\u001b[39m [seizure_label]\n\u001b[1;32m     72\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mlen\u001b[39m(new_row))\n\u001b[0;32m---> 73\u001b[0m \u001b[43mnew_train_df\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloc\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mnew_train_df\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;241m=\u001b[39m new_row\n\u001b[1;32m     74\u001b[0m count\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[1;32m     75\u001b[0m \u001b[38;5;28mprint\u001b[39m(count)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pandas/core/indexing.py:911\u001b[0m, in \u001b[0;36m_LocationIndexer.__setitem__\u001b[0;34m(self, key, value)\u001b[0m\n\u001b[1;32m    908\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_has_valid_setitem_indexer(key)\n\u001b[1;32m    910\u001b[0m iloc \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124miloc\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj\u001b[38;5;241m.\u001b[39miloc\n\u001b[0;32m--> 911\u001b[0m \u001b[43miloc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_setitem_with_indexer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindexer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pandas/core/indexing.py:1932\u001b[0m, in \u001b[0;36m_iLocIndexer._setitem_with_indexer\u001b[0;34m(self, indexer, value, name)\u001b[0m\n\u001b[1;32m   1929\u001b[0m     indexer, missing \u001b[38;5;241m=\u001b[39m convert_missing_indexer(indexer)\n\u001b[1;32m   1931\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m missing:\n\u001b[0;32m-> 1932\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_setitem_with_indexer_missing\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindexer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1933\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m   1935\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mloc\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m   1936\u001b[0m     \u001b[38;5;66;03m# must come after setting of missing\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pandas/core/indexing.py:2306\u001b[0m, in \u001b[0;36m_iLocIndexer._setitem_with_indexer_missing\u001b[0;34m(self, indexer, value)\u001b[0m\n\u001b[1;32m   2303\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_list_like_indexer(value):\n\u001b[1;32m   2304\u001b[0m         \u001b[38;5;66;03m# must have conforming columns\u001b[39;00m\n\u001b[1;32m   2305\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(value) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj\u001b[38;5;241m.\u001b[39mcolumns):\n\u001b[0;32m-> 2306\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcannot set a row with mismatched columns\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   2308\u001b[0m     value \u001b[38;5;241m=\u001b[39m Series(value, index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj\u001b[38;5;241m.\u001b[39mcolumns, name\u001b[38;5;241m=\u001b[39mindexer)\n\u001b[1;32m   2310\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj):\n\u001b[1;32m   2311\u001b[0m     \u001b[38;5;66;03m# We will ignore the existing dtypes instead of using\u001b[39;00m\n\u001b[1;32m   2312\u001b[0m     \u001b[38;5;66;03m#  internals.concat logic\u001b[39;00m\n","\u001b[0;31mValueError\u001b[0m: cannot set a row with mismatched columns"],"ename":"ValueError","evalue":"cannot set a row with mismatched columns","output_type":"error"}]},{"cell_type":"code","source":"print(new_train_df)","metadata":{"execution":{"iopub.status.busy":"2024-04-07T12:51:00.879008Z","iopub.execute_input":"2024-04-07T12:51:00.879425Z","iopub.status.idle":"2024-04-07T12:51:00.888322Z","shell.execute_reply.started":"2024-04-07T12:51:00.879391Z","shell.execute_reply":"2024-04-07T12:51:00.887379Z"},"trusted":true},"execution_count":23,"outputs":[{"name":"stdout","text":"Empty DataFrame\nColumns: [eeg_id, eeg_sub_id, patient_id, Standard Deviation (STD), Inter-Quartile Range (IQR), Skewness, Kurtosis, Number of Zero Crossings, Hjorth Mobility, Hjorth Complexity, Higuchi Fractal Dimension, Shannon Entropy, Spectral Entropy, Binned Entropy, Delta Power, Theta Power, Alpha Power, Beta Power, Gamma Power, Delta/Theta Ratio, Delta/Alpha Ratio, Theta/Beta Ratio, Alpha/Gamma Ratio, Beta/Gamma Ratio, Delta+Theta Power, Alpha+Beta Power, Theta+Gamma Power, Total Power, expert_consensus]\nIndex: []\n\n[0 rows x 29 columns]\n","output_type":"stream"}]},{"cell_type":"code","source":"import pandas as pd\n\n# Load the CSV file\ndf = pd.read_csv('/kaggle/input/hms-harmful-brain-activity-classification/train.csv')\n\n# Calculate the percentage of each label\nlabel_percentage = df['expert_consensus'].value_counts(normalize=True) * 100\n\nprint(label_percentage)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Model Architure\n","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}