{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":59093,"databundleVersionId":7469972,"sourceType":"competition"},{"sourceId":8064829,"sourceType":"datasetVersion","datasetId":4757865}],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"Imports\n","metadata":{}},{"cell_type":"code","source":"!pip install antropy\n!pip install catboost\nimport torch\nimport catboost \nfrom catboost import CatBoostClassifier, Pool\nimport mne\n\nimport antropy\n\n\nfrom sklearn.preprocessing import label_binarize\nfrom IPython.display import clear_output\n\nmne.set_log_level('ERROR')\n\nif torch.cuda.is_available():\n    device = \"cuda\"\nelse:\n    device = \"cpu\"\nimport pandas as pd\n\nimport numpy as np\nfrom scipy.fftpack import fft, ifft\nfrom scipy.stats import entropy\nfrom sklearn.preprocessing import LabelEncoder\nfrom scipy.stats import skew\nfrom scipy.stats import kurtosis\n#NOTE USE LABEL ENCODER BEFORE RUNNING THE FULL VERSION \nclear_output()","metadata":{"execution":{"iopub.status.busy":"2024-04-08T15:15:31.526147Z","iopub.execute_input":"2024-04-08T15:15:31.527034Z","iopub.status.idle":"2024-04-08T15:15:55.542841Z","shell.execute_reply.started":"2024-04-08T15:15:31.527001Z","shell.execute_reply":"2024-04-08T15:15:55.541603Z"},"trusted":true},"execution_count":28,"outputs":[]},{"cell_type":"markdown","source":"Dataset and preprocessing\n","metadata":{}},{"cell_type":"code","source":"#this is where we actually extract the features \ndef get_variability_measures(eeg_data):\n    variability_features=[]\n    std_value=np.std(eeg_data,axis=1)\n    iqr_value = np.subtract(*np.percentile(eeg_data, [75, 25], axis=1))  # IQR of all channels\n    variability_features.extend((np.mean(std_value),np.mean(iqr_value)))\n    return variability_features\n\ndef get_distribution_features(eeg_data):\n    distribution_features=[]\n    skewness_value = skew(eeg_data, axis=1)\n    kurtosis_value = kurtosis(eeg_data, axis=1) \n    distribution_features.extend((np.mean(skewness_value),np.mean(kurtosis_value)))\n    return distribution_features\n  \ndef zero_crossings(signal):\n    # If signal crosses zero line, we'll have a change in sign of adjacent values\n    return ((signal[:-1] * signal[1:]) < 0).sum()\n\ndef frequency_content_features(eeg_data):\n    # Calculate zero crossings for each channel\n    zero_crossings_values = np.apply_along_axis(zero_crossings, 1, eeg_data)\n    \n    mean_zero_crossings = np.mean(zero_crossings_values)\n    return mean_zero_crossings\n\ndef hjorth_mobility(signal):\n    first_derivative = np.diff(signal)\n    variance = np.var(signal)\n    variance_derivative = np.var(first_derivative)\n    mobility = np.sqrt(variance_derivative / variance)\n    return mobility\n\ndef get_hjorth_parameters(eeg_data):\n    hjorth_parameters=[]\n    signal=eeg_data\n    \n    \n    \n    #hjorth complexity\n    first_derivative = np.diff(signal)\n    second_derivative = np.diff(first_derivative)\n    mobility = hjorth_mobility(signal)\n    mobility_derivative = hjorth_mobility(first_derivative)\n    complexity = mobility_derivative / mobility\n    hjorth_parameters.extend((mobility,complexity))\n    return hjorth_parameters\n\ndef get_higuchi_fractal_dimension(eeg_data):\n    signal=eeg_data\n    kmax=6\n    Lk = []\n    for k in range(1, kmax):\n        Lm = []\n        for m in range(0, k):\n            Lmk = 0\n            for i in range(1, int(np.floor((len(signal) - m) / k))):\n                Lmk += abs(signal[m + i * k] - signal[m + i * k - k])\n            Lmk = Lmk * (len(signal) / (k * int(np.floor((len(signal) - m) / k))))\n            Lm.append(Lmk)\n        Lk.append(np.log(np.mean(Lm)))\n    (slope, _) = np.polyfit(np.log(range(1, kmax)), Lk, 1)\n    return -slope\n\ndef get_entropies(eeg_data):\n    entropies=[]\n    signal=eeg_data\n    #shanon entropy\n    hist, _ = np.histogram(signal, bins=64, density=True)\n    shannon_entropy=entropy(hist)\n    #spectral entropy\n    spectral_entropy=antropy.spectral_entropy(eeg_data,200,'welch',nperseg=None,normalize=True)s\n    \n    #binned_entorpy\n    hist, _ = np.histogram(signal, bins=10)\n    binned_entropy=entropy(hist)\n    entropies.extend((shannon_entropy,spectral_entropy,binned_entropy))\n    return entropies\n    \ndef get_power_bands(new_raw,tmin,tmax):\n    \n    power_features=[]\n    \n\n# Define frequency bands\n    bands = {\n        'delta': (0.5, 4),\n        'theta': (4, 8),\n        'alpha': (8, 13),\n        'beta': (13, 30),\n        'gamma': (30, None)  # Assuming Gamma is 30+ Hz\n    }\n\n# Dictionary to hold power values for each band\n    band_power = {}\n\n    for band, (l_freq, h_freq) in bands.items():\n        # Filter the data for each frequency band\n        band_data = new_raw.copy().filter(l_freq=l_freq, h_freq=h_freq, picks='eeg', verbose=False)\n\n        # Compute the PSD for the filtered data using the Welch method\n        sfreq = raw.info['sfreq'] \n        fmax = min(h_freq if h_freq is not None else sfreq / 2, sfreq / 2)\n        spectrum = band_data.compute_psd(method='welch', fmin=l_freq, fmax=fmax,picks='eeg',verbose=False)\n\n        psd=spectrum.get_data()\n        # Integrate the PSD over the frequencies of interest to get the absolute power for each band\n        band_power[band] = psd.mean(axis=1).sum()\n\n\n\n    \n    power_features.extend(band_power.values())\n    \n    epsilon = 1e-6\n\n    # Calculate power ratios using values from the dictionary\n    delta_theta_ratio = band_power['delta'] / (band_power['theta'] + epsilon)\n    delta_alpha_ratio = band_power['delta'] / (band_power['alpha'] + epsilon)\n    theta_beta_ratio = band_power['theta'] / (band_power['beta'] + epsilon)\n    alpha_gamma_ratio = band_power['alpha'] / (band_power['gamma'] + epsilon)\n    beta_gamma_ratio = band_power['beta'] / (band_power['gamma'] + epsilon)\n\n    # Append the calculated ratios \n    power_features.extend([delta_theta_ratio, delta_alpha_ratio, theta_beta_ratio, alpha_gamma_ratio, beta_gamma_ratio])\n\n    # Calculate power sums using values from the dictionary\n    delta_theta_sum = band_power['delta'] + band_power['theta']\n    alpha_beta_sum = band_power['alpha'] + band_power['beta']\n    theta_gamma_sum = band_power['theta'] + band_power['gamma']\n\n    # Append the calculated sums \n    power_features.extend([delta_theta_sum, alpha_beta_sum, theta_gamma_sum])\n    total_power=band_power['delta'] + band_power['theta']+ band_power['gamma']+band_power['alpha']\n    power_features.append(total_power)\n    return power_features\n    \n\n    \n\n    ","metadata":{"execution":{"iopub.status.busy":"2024-04-08T05:34:39.505773Z","iopub.execute_input":"2024-04-08T05:34:39.506256Z","iopub.status.idle":"2024-04-08T05:34:39.531561Z","shell.execute_reply.started":"2024-04-08T05:34:39.506229Z","shell.execute_reply":"2024-04-08T05:34:39.530456Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"\n    \ndef extract_features(segment, segment_info,tmin,tmax):\n    features = []\n    eeg_data = np.real(segment.get_data())\n    new_raw = mne.io.RawArray(eeg_data, segment_info)\n    \n    \n    features.extend(get_variability_measures(eeg_data))\n    features.extend(get_distribution_features(eeg_data))\n    \n    \n    zero_crossing_feature = [frequency_content_features(eeg_data)]\n    features.extend(zero_crossing_feature)\n    \n    # Extend with placeholders for missing features (Hjorth Mobility, etc.)\n    features.extend(get_hjorth_parameters(eeg_data)) \n    \n    higuchi_fractal_dimension=[get_higuchi_fractal_dimension(eeg_data)]\n    features.extend(higuchi_fractal_dimension)\n    \n    features.extend(get_entropies(eeg_data))\n    power_band_features = get_power_bands(new_raw,tmin,tmax)\n    features.extend(power_band_features)\n    \n    \n    \n    \n    return features\n\n\ndef create_raw_from_parquet(parquet_file):\n    df=pd.read_parquet(parquet_file)\n    data=df.to_numpy().T\n    info=mne.create_info(ch_names=list(df.columns),sfreq=200,ch_types='eeg')\n    raw=mne.io.RawArray(data,info)\n    return raw\n\ndef get_duration(raw):\n    num_samples=len(raw.times)\n    sampling_freq=raw.info['sfreq']\n    duration=np.floor(num_samples/sampling_freq)\n    \n    return duration","metadata":{"execution":{"iopub.status.busy":"2024-04-08T05:34:45.968683Z","iopub.execute_input":"2024-04-08T05:34:45.969039Z","iopub.status.idle":"2024-04-08T05:34:45.978797Z","shell.execute_reply.started":"2024-04-08T05:34:45.969010Z","shell.execute_reply":"2024-04-08T05:34:45.977872Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"\n\n\n# Initialize an empty DataFrame for the new training data\ncolumns = [\n    'eeg_id', \n    'eeg_sub_id', \n    'patient_id', \n    'Standard Deviation (STD)', \n    'Inter-Quartile Range (IQR)', \n    'Skewness', \n    'Kurtosis', \n    'Number of Zero Crossings', \n    'Hjorth Mobility', \n    'Hjorth Complexity', \n    'Higuchi Fractal Dimension', \n    'Shannon Entropy', \n    'Spectral Entropy', \n    'Binned Entropy', \n    'Delta Power', \n    'Theta Power', \n    'Alpha Power', \n    'Beta Power', \n    'Gamma Power', \n    'Delta/Theta Ratio', \n    'Delta/Alpha Ratio', \n    'Theta/Beta Ratio', \n    'Alpha/Gamma Ratio', \n    'Beta/Gamma Ratio', \n    'Delta+Theta Power', \n    'Alpha+Beta Power', \n    'Theta+Gamma Power', \n    'Total Power',\n    'expert_consensus'\n]\n\nnew_train_df = pd.DataFrame(columns=columns)\n#print(new_train_df.shape[1])\nproblematic_files = []\n# Load the original training CSV\ntrain_csv = pd.read_csv('/kaggle/input/hms-harmful-brain-activity-classification/train.csv')\ncount=0\n# Loop through each row in the train CSV\nfor index, row in train_csv.iterrows():\n    try:\n        eeg_id = row['eeg_id']\n        sub_id = row['eeg_sub_id']\n        patient_id = row['patient_id']\n        seizure_label = row['expert_consensus']\n\n       \n\n        # Load the corresponding parquet file as an mne Raw object\n        raw = create_raw_from_parquet(f'/kaggle/input/hms-harmful-brain-activity-classification/train_eegs/{eeg_id}.parquet')\n        \n\n        sub_id_start_time=sub_id\n        tmin = sub_id_start_time * 50\n        tmax = tmin + 50  # This ensures a 50-second window\n\n        # Adjust tmax to not exceed the recording\n        tmax = min(tmax, raw.times[-1])\n\n        # Additionally, ensure tmin does not exceed the adjusted tmax or the recording\n        tmin = min(tmin, tmax - 0.001)\n        segment = raw.copy().crop(tmin, tmax)  # Adjust 'tmin' and 'tmax' as necessary\n        segment_info=raw.info\n\n\n        # Extract features from the surrogate segment\n        features = extract_features(segment,segment_info,tmin,tmax)\n        #print(len(features))\n        # Append to the new DataFrame\n        new_row = [eeg_id, sub_id, patient_id] + features + [seizure_label]\n        #print(len(new_row))\n        new_train_df.loc[len(new_train_df)] = new_row\n        count+=1\n        print(count)\n        clear_output()\n    except :\n        pass\n        continue\n    \n# Save the new DataFrame to CSV\nnew_train_df.to_csv('feature_extracted_data.csv', index=False)\n","metadata":{"execution":{"iopub.status.busy":"2024-04-08T05:34:50.037803Z","iopub.execute_input":"2024-04-08T05:34:50.038636Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stdout","text":"2457\n2457\n","output_type":"stream"}]},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"import pandas as pd\n\n# Load the CSV file\ndf = pd.read_csv('/kaggle/input/hms-harmful-brain-activity-classification/train.csv')\n\n# Calculate the percentage of each label\nlabel_percentage = df['expert_consensus'].value_counts(normalize=True) * 100\n\nprint(label_percentage)\n","metadata":{"execution":{"iopub.status.busy":"2024-04-08T15:11:22.926354Z","iopub.execute_input":"2024-04-08T15:11:22.927096Z","iopub.status.idle":"2024-04-08T15:11:23.177248Z","shell.execute_reply.started":"2024-04-08T15:11:22.927065Z","shell.execute_reply":"2024-04-08T15:11:23.176320Z"},"trusted":true},"execution_count":25,"outputs":[{"name":"stdout","text":"expert_consensus\nSeizure    19.600187\nGRDA       17.660112\nOther      17.610487\nGPD        15.638577\nLRDA       15.580524\nLPD        13.910112\nName: proportion, dtype: float64\n","output_type":"stream"}]},{"cell_type":"markdown","source":"Model Architure\n","metadata":{}},{"cell_type":"code","source":"df=pd.read_csv('/kaggle/input/feature-extracted-data/feature_extracted_data.csv')\ndf['Spectral Entropy'] = df['Spectral Entropy'].apply(lambda x: np.fromstring(x.strip('[]'), sep=' '))\ndf['Spectral Entropy'] = df['Spectral Entropy'].apply(lambda x: np.mean(x))\ntrain_data=df.iloc[:, 3:-1]\n\n\ntest_data = catboost_pool = Pool(train_data, \n                                 train_labels)\n\n\n\n","metadata":{"execution":{"iopub.status.busy":"2024-04-08T15:09:01.454163Z","iopub.execute_input":"2024-04-08T15:09:01.454523Z","iopub.status.idle":"2024-04-08T15:09:02.364637Z","shell.execute_reply.started":"2024-04-08T15:09:01.454495Z","shell.execute_reply":"2024-04-08T15:09:02.363703Z"},"trusted":true},"execution_count":22,"outputs":[{"name":"stdout","text":"object\n","output_type":"stream"}]},{"cell_type":"code","source":"model = CatBoostClassifier(\n    iterations=1000,\n    learning_rate=0.1,\n    depth=6,\n    #eval_metric='MultiCrossEntropy',\n    loss_function='MultiClass',\n    auto_class_weights='Balanced',\n    bootstrap_type='Bernoulli',\n    subsample=0.8,\n    thread_count=-1,\n    task_type='GPU',  # Use 'CPU' if GPU is not available\n    early_stopping_rounds=50\n   \n    # Add any other specific parameters here\n)\n\n\nmodel.fit(train_data, train_labels)\nclear_output()\n# make the prediction using the resulting model","metadata":{"execution":{"iopub.status.busy":"2024-04-08T15:17:57.924768Z","iopub.execute_input":"2024-04-08T15:17:57.925437Z","iopub.status.idle":"2024-04-08T15:18:03.802605Z","shell.execute_reply.started":"2024-04-08T15:17:57.925403Z","shell.execute_reply":"2024-04-08T15:18:03.801560Z"},"trusted":true},"execution_count":30,"outputs":[]},{"cell_type":"code","source":"preds_class = model.predict(test_data)\npreds_proba = model.predict_proba(test_data)\nprint(\"class = \", preds_class)\nprint(\"proba = \", preds_proba)","metadata":{"execution":{"iopub.status.busy":"2024-04-08T15:18:57.260962Z","iopub.execute_input":"2024-04-08T15:18:57.261974Z","iopub.status.idle":"2024-04-08T15:18:57.430319Z","shell.execute_reply.started":"2024-04-08T15:18:57.261932Z","shell.execute_reply":"2024-04-08T15:18:57.429380Z"},"trusted":true},"execution_count":31,"outputs":[{"name":"stdout","text":"class =  [['Seizure']\n ['Seizure']\n ['GPD']\n ...\n ['LRDA']\n ['LRDA']\n ['LRDA']]\nproba =  [[0.04160568 0.11218301 0.02271567 0.1460505  0.14286188 0.53458326]\n [0.01151042 0.10234031 0.05255811 0.07861264 0.14368232 0.6112962 ]\n [0.49189439 0.07798545 0.04437758 0.07506577 0.27952117 0.03115563]\n ...\n [0.07991551 0.09472494 0.28666442 0.34920367 0.08601937 0.10347209]\n [0.01197037 0.2349718  0.0595463  0.6332093  0.04815713 0.0121451 ]\n [0.03949748 0.09521253 0.18773968 0.54395361 0.11320222 0.02039448]]\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Submission","metadata":{}},{"cell_type":"code","source":"columns=['eeg_id','seizure_vote','lpd_vote','gpd_vote','lrda_vote','grda_vote','other_vote']\nsubmission_csv=pd.Dataframe(columns=columns)\n\ncolumns = [\n    'eeg_id', \n    'eeg_sub_id', \n    'patient_id', \n    'Standard Deviation (STD)', \n    'Inter-Quartile Range (IQR)', \n    'Skewness', \n    'Kurtosis', \n    'Number of Zero Crossings', \n    'Hjorth Mobility', \n    'Hjorth Complexity', \n    'Higuchi Fractal Dimension', \n    'Shannon Entropy', \n    'Spectral Entropy', \n    'Binned Entropy', \n    'Delta Power', \n    'Theta Power', \n    'Alpha Power', \n    'Beta Power', \n    'Gamma Power', \n    'Delta/Theta Ratio', \n    'Delta/Alpha Ratio', \n    'Theta/Beta Ratio', \n    'Alpha/Gamma Ratio', \n    'Beta/Gamma Ratio', \n    'Delta+Theta Power', \n    'Alpha+Beta Power', \n    'Theta+Gamma Power', \n    'Total Power',\n    'expert_consensus'\n]\n\nnew_train_df = pd.DataFrame(columns=columns)\n#print(new_train_df.shape[1])\nproblematic_files = []\n# Load the original training CSV\ntest_csv = pd.read_csv('/kaggle/input/hms-harmful-brain-activity-classification/test.csv')\ncount=0\n# Loop through each row in the train CSV\nfor index, row in train_csv.iterrows():\n    try:\n        eeg_id = row['eeg_id']\n        sub_id = 0\n        patient_id = row['patient_id']\n        seizure_label = row['expert_consensus']\n\n       \n\n        # Load the corresponding parquet file as an mne Raw object\n        raw = create_raw_from_parquet(f'/kaggle/input/hms-harmful-brain-activity-classification/test_eegs/{eeg_id}.parquet')\n        \n\n        sub_id_start_time=sub_id\n        tmin = sub_id_start_time * 50\n        tmax = tmin + 50  # This ensures a 50-second window\n\n        # Adjust tmax to not exceed the recording\n        tmax = min(tmax, raw.times[-1])\n\n        # Additionally, ensure tmin does not exceed the adjusted tmax or the recording\n        tmin = min(tmin, tmax - 0.001)\n        segment = raw.copy().crop(tmin, tmax)  # Adjust 'tmin' and 'tmax' as necessary\n        segment_info=raw.info\n\n\n        # Extract features from the surrogate segment\n        features = extract_features(segment,segment_info,tmin,tmax)\n        #print(len(features))\n        # Append to the new DataFrame\n        new_row = [eeg_id, sub_id, patient_id] + features + [seizure_label]\n        #print(len(new_row))\n        new_train_df.loc[len(new_train_df)] = new_row\n        count+=1\n        print(count)\n        clear_output()\n    except :\n        pass\n        continue\n    \n# Save the new DataFrame to CSV\nnew_train_df.to_csv('feature_extracted_data.csv', index=False)\ndf=new_train_df\ndf['Spectral Entropy'] = df['Spectral Entropy'].apply(lambda x: np.fromstring(x.strip('[]'), sep=' '))\ndf['Spectral Entropy'] = df['Spectral Entropy'].apply(lambda x: np.mean(x))\ntrain_data=df.iloc[:, 3:-1]\nmodel.predict(df)","metadata":{"execution":{"iopub.status.busy":"2024-04-08T15:20:01.985068Z","iopub.execute_input":"2024-04-08T15:20:01.985824Z","iopub.status.idle":"2024-04-08T15:20:02.016929Z","shell.execute_reply.started":"2024-04-08T15:20:01.985794Z","shell.execute_reply":"2024-04-08T15:20:02.015806Z"},"trusted":true},"execution_count":32,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","Cell \u001b[0;32mIn[32], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mget_evals_result\u001b[49m()\n","\u001b[0;31mNameError\u001b[0m: name 'get_evals_result' is not defined"],"ename":"NameError","evalue":"name 'get_evals_result' is not defined","output_type":"error"}]},{"cell_type":"code","source":"preds_class = model.predict(test_data)\npreds_proba = model.predict_proba(test_data)\nprint(\"class = \", preds_class)\nprint(\"proba = \", preds_proba)","metadata":{},"execution_count":null,"outputs":[]}]}